# Script para tratamiento avanzado de datos inconsistentes con t√©cnicas estad√≠sticas robustas
# -------------------------------------------------------------
# tratamiento_inconsistencias.py
# M√≥dulo especializado para el tratamiento de inconsistencias usando t√©cnicas estad√≠sticas avanzadas.
#
# Autor: Juan Camilo Ria√±o Molano
# Fecha de creaci√≥n: 01/08/2025
# Descripci√≥n:
#   Este script implementa t√©cnicas estad√≠sticas robustas para el tratamiento de datos inconsistentes:
#   - Eliminaci√≥n inteligente de duplicados con m√∫ltiples criterios
#   - Imputaci√≥n de precios faltantes por mediana zonal con validaci√≥n
#   - Winsorizaci√≥n de outliers al 1% para preservar distribuciones
#   - Tratamiento espec√≠fico por tipo de inconsistencia detectada
#   - Logging detallado de todas las transformaciones aplicadas
#   - Backup autom√°tico antes de aplicar cambios
#
#   Se ejecuta despu√©s del an√°lisis pre-limpieza y antes de la limpieza final,
#   aplicando t√©cnicas estad√≠sticas que mejoran la calidad sin distorsionar patrones.
#
# Funcionalidades principales:
#   - Eliminaci√≥n de duplicados exactos y similares con umbral configurable
#   - Imputaci√≥n zonal por mediana para preservar patrones geogr√°ficos
#   - Winsorizaci√≥n configurable (default 1%) para manejo robusto de outliers
#   - Validaci√≥n post-tratamiento para verificar mejoras en calidad
#   - M√©tricas de impacto y reportes de transformaciones aplicadas
#   - Integraci√≥n seamless con pipeline existente sin afectar flujo
#
# Buenas pr√°cticas implementadas:
#   - Backup autom√°tico antes de cualquier transformaci√≥n destructiva
#   - Logging detallado con m√©tricas before/after para auditabilidad
#   - Validaci√≥n de par√°metros y datos de entrada antes del procesamiento
#   - Manejo robusto de casos edge y valores extremos
#   - Configuraci√≥n flexible de umbrales y par√°metros de tratamiento
# -------------------------------------------------------------

# =======================
# Importaci√≥n de librer√≠as
# =======================
import pandas as pd
import numpy as np
import shutil
from datetime import datetime
from pathlib import Path
import warnings

warnings.filterwarnings("ignore")

# =======================
# Configuraci√≥n de rutas
# =======================
BASE_DIR = Path(__file__).parent.parent
PROCESSED_DIR = BASE_DIR / "data" / "processedData"
BACKUP_DIR = BASE_DIR / "dataBackup" / "tratamiento_inconsistencias"


def crear_backup_tratamiento():
    """
    Crea backup espec√≠fico antes del tratamiento de inconsistencias.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = BACKUP_DIR / f"backup_pre_tratamiento_{timestamp}"

    try:
        # Crear directorio de backup si no existe
        backup_path.mkdir(parents=True, exist_ok=True)

        # Backup de archivos procesados
        if PROCESSED_DIR.exists():
            shutil.copytree(PROCESSED_DIR, backup_path / "processedData", dirs_exist_ok=True)
            print(f"‚úì Backup creado en: {backup_path}")
            return True
    except Exception as e:
        print(f"‚ö†Ô∏è Error creando backup: {e}")
        return False


def eliminar_duplicados_avanzado(df, columnas_clave=None, umbral_similitud=0.95):
    """
    Elimina duplicados SOLO en columnas cr√≠ticas: ID y Fecha_Actualizacion.

    Todas las dem√°s columnas pueden tener duplicados ya que es comportamiento
    normal del negocio inmobiliario (m√∫ltiples propiedades similares).

    Args:
        df (DataFrame): Dataset a procesar
        columnas_clave (list): Columnas cr√≠ticas para identificar duplicados
        umbral_similitud (float): No utilizado en esta versi√≥n

    Returns:
        DataFrame: Dataset sin duplicados cr√≠ticos
        dict: Estad√≠sticas del proceso
    """
    print("üîÑ Eliminando duplicados solo en columnas cr√≠ticas...")
    registros_iniciales = len(df)

    # Identificar columnas cr√≠ticas que NO deben tener duplicados
    columnas_criticas = []

    # Solo verificar ID si existe en el dataset
    if "Id" in df.columns:
        columnas_criticas.append("Id")
    elif "ID" in df.columns:
        columnas_criticas.append("ID")
    elif "Inmueble_ID" in df.columns:
        columnas_criticas.append("Inmueble_ID")

    # Solo verificar Fecha_Actualizacion si existe
    if "Fecha_Actualizacion" in df.columns:
        columnas_criticas.append("Fecha_Actualizacion")

    if not columnas_criticas:
        print("  ‚úì No se encontraron columnas cr√≠ticas para verificar duplicados")
        return df, {
            "registros_iniciales": registros_iniciales,
            "registros_finales": len(df),
            "eliminados": 0,
            "porcentaje_eliminado": 0.0,
        }

    print(f"  ‚úì Verificando duplicados en columnas cr√≠ticas: {columnas_criticas}")

    # Eliminar duplicados solo en columnas cr√≠ticas
    duplicados_criticos = df.duplicated(subset=columnas_criticas, keep="first")
    df_limpio = df[~duplicados_criticos].copy()
    criticos_eliminados = duplicados_criticos.sum()

    registros_finales = len(df_limpio)
    porcentaje_eliminado = (criticos_eliminados / registros_iniciales) * 100

    print(f"  ‚úì Duplicados cr√≠ticos eliminados: {criticos_eliminados}")
    print(f"  ‚úì Total eliminados: {criticos_eliminados} ({porcentaje_eliminado:.2f}%)")

    # Estad√≠sticas de retorno
    estadisticas = {
        "registros_iniciales": registros_iniciales,
        "duplicados_criticos_eliminados": criticos_eliminados,
        "registros_finales": registros_finales,
        "porcentaje_eliminado": porcentaje_eliminado,
    }

    return df_limpio, estadisticas


def imputar_precios_mediana_zonal(df, columna_precio="Precio_Solicitado", columna_zona="Zona"):
    """
    Imputa precios faltantes usando la mediana por zona.

    Args:
        df (DataFrame): Dataset a procesar
        columna_precio (str): Nombre de la columna de precio
        columna_zona (str): Nombre de la columna de zona

    Returns:
        DataFrame: Dataset con precios imputados
        dict: Estad√≠sticas del proceso
    """
    print("üí∞ Imputando precios faltantes por mediana zonal...")
    df_procesado = df.copy()

    # Convertir a num√©rico
    df_procesado[columna_precio] = pd.to_numeric(df_procesado[columna_precio], errors="coerce")

    # Identificar valores faltantes
    valores_faltantes = df_procesado[columna_precio].isna()
    total_faltantes = valores_faltantes.sum()

    if total_faltantes == 0:
        print("  ‚úì No hay valores faltantes en precios")
        return df_procesado, {"valores_imputados": 0, "porcentaje_imputado": 0}

    # Calcular mediana por zona
    mediana_por_zona = df_procesado.groupby(columna_zona)[columna_precio].median()

    # Imputar valores faltantes
    imputaciones_realizadas = 0
    for zona in df_procesado[columna_zona].unique():
        if zona in mediana_por_zona.index and not pd.isna(mediana_por_zona[zona]):
            mask = (df_procesado[columna_zona] == zona) & valores_faltantes
            valores_a_imputar = mask.sum()

            if valores_a_imputar > 0:
                df_procesado.loc[mask, columna_precio] = mediana_por_zona[zona]
                imputaciones_realizadas += valores_a_imputar
                print(f"  ‚úì Zona '{zona}': {valores_a_imputar} valores imputados con mediana ${mediana_por_zona[zona]:,.0f}")

    # Para valores que no pudieron ser imputados por zona, usar mediana global
    valores_aun_faltantes = df_procesado[columna_precio].isna().sum()
    if valores_aun_faltantes > 0:
        mediana_global = df_procesado[columna_precio].median()
        df_procesado[columna_precio] = df_procesado[columna_precio].fillna(mediana_global)
        imputaciones_realizadas += valores_aun_faltantes
        print(f"  ‚úì {valores_aun_faltantes} valores imputados con mediana global ${mediana_global:,.0f}")

    estadisticas = {
        "valores_faltantes_iniciales": total_faltantes,
        "valores_imputados": imputaciones_realizadas,
        "porcentaje_imputado": (total_faltantes / len(df)) * 100,
        "mediana_por_zona": mediana_por_zona.to_dict(),
    }

    print(f"  ‚úì Total imputado: {imputaciones_realizadas} valores ({estadisticas['porcentaje_imputado']:.2f}%)")

    return df_procesado, estadisticas


def winsorizar_outliers(df, columnas_numericas=None, percentil_inferior=1, percentil_superior=99):
    """
    Aplica winsorizaci√≥n a outliers en columnas num√©ricas.

    Args:
        df (DataFrame): Dataset a procesar
        columnas_numericas (list): Columnas a winsorizar
        percentil_inferior (float): Percentil inferior para winsorizaci√≥n
        percentil_superior (float): Percentil superior para winsorizaci√≥n

    Returns:
        DataFrame: Dataset winsorizado
        dict: Estad√≠sticas del proceso
    """
    print(f"üìà Aplicando winsorizaci√≥n de outliers (P{percentil_inferior}-P{percentil_superior})...")
    df_procesado = df.copy()

    # Columnas por defecto
    if columnas_numericas is None:
        columnas_numericas = ["Precio_Solicitado", "Area"]

    estadisticas = {}

    for columna in columnas_numericas:
        if columna not in df.columns:
            continue

        # Convertir a num√©rico
        df_procesado[columna] = pd.to_numeric(df_procesado[columna], errors="coerce")
        valores_numericos = df_procesado[columna].dropna()

        if len(valores_numericos) == 0:
            continue

        # Calcular percentiles
        p_inferior = np.percentile(valores_numericos, percentil_inferior)
        p_superior = np.percentile(valores_numericos, percentil_superior)

        # Identificar outliers
        outliers_inferiores = (df_procesado[columna] < p_inferior) & df_procesado[columna].notna()
        outliers_superiores = (df_procesado[columna] > p_superior) & df_procesado[columna].notna()

        total_outliers = outliers_inferiores.sum() + outliers_superiores.sum()

        if total_outliers > 0:
            # Aplicar winsorizaci√≥n
            df_procesado.loc[outliers_inferiores, columna] = p_inferior
            df_procesado.loc[outliers_superiores, columna] = p_superior

            estadisticas[columna] = {
                "outliers_inferiores": outliers_inferiores.sum(),
                "outliers_superiores": outliers_superiores.sum(),
                "total_outliers": total_outliers,
                "percentil_inferior": p_inferior,
                "percentil_superior": p_superior,
                "porcentaje_outliers": (total_outliers / len(valores_numericos)) * 100,
            }

            print(f"  ‚úì {columna}: {total_outliers} outliers winsorizados ({estadisticas[columna]['porcentaje_outliers']:.2f}%)")
            print(f"    - L√≠mites: [{p_inferior:,.0f}, {p_superior:,.0f}]")
        else:
            estadisticas[columna] = {"total_outliers": 0, "porcentaje_outliers": 0}
            print(f"  ‚úì {columna}: No se detectaron outliers")

    return df_procesado, estadisticas


def validar_mejoras_calidad(df_original, df_tratado):
    """
    Valida que el tratamiento mejor√≥ la calidad de los datos.

    Args:
        df_original (DataFrame): Dataset original
        df_tratado (DataFrame): Dataset despu√©s del tratamiento

    Returns:
        dict: M√©tricas de mejora en calidad
    """
    print("‚úÖ Validando mejoras en calidad de datos...")

    # M√©tricas de completitud
    completitud_original = (1 - df_original.isnull().sum() / len(df_original)) * 100
    completitud_tratado = (1 - df_tratado.isnull().sum() / len(df_tratado)) * 100

    # M√©tricas de consistencia (coeficiente de variaci√≥n)
    cv_original = {}
    cv_tratado = {}

    for col in ["Precio_Solicitado", "Area"]:
        if col in df_original.columns:
            # Original
            datos_orig = pd.to_numeric(df_original[col], errors="coerce").dropna()
            if len(datos_orig) > 0 and datos_orig.mean() != 0:
                cv_original[col] = datos_orig.std() / datos_orig.mean()

            # Tratado
            datos_trat = pd.to_numeric(df_tratado[col], errors="coerce").dropna()
            if len(datos_trat) > 0 and datos_trat.mean() != 0:
                cv_tratado[col] = datos_trat.std() / datos_trat.mean()

    mejoras = {
        "registros_original": len(df_original),
        "registros_tratado": len(df_tratado),
        "completitud_original": completitud_original.to_dict(),
        "completitud_tratado": completitud_tratado.to_dict(),
        "coef_variacion_original": cv_original,
        "coef_variacion_tratado": cv_tratado,
    }

    print(f"  ‚úì Registros: {len(df_original)} ‚Üí {len(df_tratado)}")

    for col in completitud_original.index:
        if col in completitud_tratado.index:
            mejora = completitud_tratado[col] - completitud_original[col]
            print(f"  ‚úì Completitud {col}: {completitud_original[col]:.1f}% ‚Üí {completitud_tratado[col]:.1f}% ({mejora:+.1f}%)")

    return mejoras


def ejecutar_tratamiento_inconsistencias():
    """
    Funci√≥n principal que ejecuta todo el tratamiento de inconsistencias.
    """
    print("=" * 80)
    print("INICIANDO TRATAMIENTO AVANZADO DE DATOS INCONSISTENTES")
    print("=" * 80)

    # Rutas de archivos
    archivo_muestra = PROCESSED_DIR / "muestra.csv"

    # Verificar existencia de archivos
    if not archivo_muestra.exists():
        print("‚ö†Ô∏è No se encontr√≥ archivo de muestra procesado. Ejecute primero obtain_data.py")
        return False

    # Crear backup
    if not crear_backup_tratamiento():
        print("‚ö†Ô∏è No se pudo crear backup. Continuando...")

    try:
        # Cargar datos
        print("\nüìÇ Cargando datos procesados...")
        df_muestra = pd.read_csv(archivo_muestra)
        print(f"  ‚úì Muestra cargada: {len(df_muestra)} registros")

        df_original = df_muestra.copy()  # Para validaci√≥n final

        # 1. Eliminaci√≥n de duplicados
        print("\n" + "=" * 60)
        print("1. ELIMINACI√ìN DE DUPLICADOS")
        print("=" * 60)
        df_muestra, stats_duplicados = eliminar_duplicados_avanzado(df_muestra)

        # 2. Imputaci√≥n de precios
        print("\n" + "=" * 60)
        print("2. IMPUTACI√ìN DE PRECIOS FALTANTES")
        print("=" * 60)
        df_muestra, stats_imputacion = imputar_precios_mediana_zonal(df_muestra)

        # 3. Winsorizaci√≥n de outliers
        print("\n" + "=" * 60)
        print("3. WINSORIZACI√ìN DE OUTLIERS")
        print("=" * 60)
        df_muestra, stats_winsor = winsorizar_outliers(df_muestra)

        # 4. Validaci√≥n de mejoras
        print("\n" + "=" * 60)
        print("4. VALIDACI√ìN DE MEJORAS EN CALIDAD")
        print("=" * 60)
        validar_mejoras_calidad(df_original, df_muestra)

        # Guardar resultado
        output_path = PROCESSED_DIR / "muestra_tratada.csv"
        df_muestra.to_csv(output_path, index=False, encoding="utf-8")
        print(f"\n‚úÖ Datos tratados guardados en: {output_path}")

        # Resumen final
        print("\n" + "=" * 80)
        print("RESUMEN DE TRATAMIENTO COMPLETADO")
        print("=" * 80)
        print(f"üìä Registros procesados: {len(df_original)} ‚Üí {len(df_muestra)}")
        print(f"üîÑ Duplicados eliminados: {stats_duplicados['duplicados_criticos_eliminados']}")
        print(f"üí∞ Precios imputados: {stats_imputacion['valores_imputados']}")

        total_outliers_winsor = sum([stats["total_outliers"] for stats in stats_winsor.values()])
        print(f"üìà Outliers winsorizados: {total_outliers_winsor}")
        print("‚úÖ Tratamiento de inconsistencias completado exitosamente")
        
        # Aclaraci√≥n sobre el impacto en an√°lisis posteriores
        print("\n" + "üí°" * 50)
        print("NOTA IMPORTANTE: IMPACTO EN AN√ÅLISIS POSTERIOR")
        print("üí°" * 50)
        print("üîÑ Los datos procesados han sido mejorados estad√≠sticamente:")
        print("   ‚Ä¢ Outliers corregidos por winsorizaci√≥n (l√≠mites P1-P99)")
        print("   ‚Ä¢ Valores faltantes imputados con medianas zonales")
        print("   ‚Ä¢ Duplicados cr√≠ticos eliminados")
        print("üìä Al comparar con an√°lisis PRE-tratamiento, esperar√°:")
        print("   ‚Ä¢ MENOR n√∫mero de inconsistencias detectadas")
        print("   ‚Ä¢ MEJOR tasa de confiabilidad de datos")
        print("   ‚Ä¢ PRECIOS m√°s estables (menos outliers extremos)")
        print("   ‚Ä¢ ESTADOS m√°s consistentes despu√©s de limpieza")
        print("üí°" * 50)

        return True

    except Exception as e:
        print(f"‚ùå Error durante el tratamiento: {e}")
        return False


if __name__ == "__main__":
    ejecutar_tratamiento_inconsistencias()
